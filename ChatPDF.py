# ì„¤ì¹˜ í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬:
# pip install streamlit rank_bm25 faiss-cpu sentence-transformers langchain-huggingface pymupdf

# 1) ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import os, json, sqlite3
import streamlit as st
import pymupdf  # PyMuPDF

from langchain_community.retrievers import BM25Retriever
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.retrievers import EnsembleRetriever
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 2) DB ì„¤ì •
DB_PATH = "company_news.db"
TABLE   = "news"

# 3) DBì—ì„œ ë¬¸ì„œ ë¡œë“œ
def load_documents_from_sqlite(db_path: str):
    if not os.path.exists(db_path):
        raise FileNotFoundError(f"{db_path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    conn = sqlite3.connect(db_path)
    cur  = conn.cursor()
    cur.execute(f"SELECT id, ê¸°ì—…ëª…, ë‚ ì§œ, ë¬¸ì„œ_ì¹´í…Œê³ ë¦¬, ìš”ì•½, ì£¼ìš”_ì´ë²¤íŠ¸ FROM {TABLE} ORDER BY ë‚ ì§œ ASC")
    rows = cur.fetchall()
    conn.close()

    texts, metadatas = [], []
    for rid, company, date, category, summary, events_json in rows:
        texts.append(summary)
        try:
            events = ", ".join(json.loads(events_json))
        except Exception:
            events = events_json
        metadatas.append({
            "id": rid, "ê¸°ì—…ëª…": company, "ë‚ ì§œ": date,
            "ë¬¸ì„œ_ì¹´í…Œê³ ë¦¬": category, "ì£¼ìš”_ì´ë²¤íŠ¸": events,
            "source": f"db_doc_{rid}",
        })
    return texts, metadatas

# 4) ì•™ìƒë¸” Retriever êµ¬ì„±
def build_ensemble_retriever(texts, metadatas):
    bm25 = BM25Retriever.from_texts(texts, metadatas=metadatas); bm25.k = 2
    embedding = HuggingFaceEmbeddings(model_name="distiluse-base-multilingual-cased-v1")
    faiss_store = FAISS.from_texts(texts, embedding, metadatas=metadatas)
    faiss = faiss_store.as_retriever(search_kwargs={"k": 2})
    return EnsembleRetriever(retrievers=[bm25, faiss], weights=[0.3, 0.7])

# 5) OpenAI LLM ì´ˆê¸°í™”
@st.cache_resource(show_spinner=False)
def load_openai_llm(model_name: str = "gpt-4o-mini", temperature: float = 0.0):
    api_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY", None)
    if not api_key:
        raise RuntimeError("OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
    return ChatOpenAI(model=model_name, temperature=temperature, api_key=api_key)

# 6) ê²€ìƒ‰ í•¨ìˆ˜
def search(query: str, retriever):
    docs = retriever.invoke(query)
    return docs or []

# 7) í”„ë¡¬í”„íŠ¸ êµ¬ì„±
def build_prompt(query: str, docs):
    lines = []
    lines.append("ì•„ë˜ 'ìë£Œ'ë§Œ ê·¼ê±°ë¡œ í•œêµ­ì–´ë¡œ ê°„ê²°íˆ ë‹µí•˜ì„¸ìš”.")
    lines.append("- ìë£Œ ë°– ì •ë³´ë¥¼ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.")
    lines.append("- ë‹µí•  ìˆ˜ ì—†ìœ¼ë©´ 'ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'ë¼ê³  ë§í•˜ì„¸ìš”.\n")
    lines.append(f"ì§ˆë¬¸:\n{query}\n")
    lines.append("ìë£Œ:")
    for i, d in enumerate(docs, 1):
        m = d.metadata
        lines.append(
            f"[ë¬¸ì„œ{i}] (source={m.get('source')}, ê¸°ì—…ëª…={m.get('ê¸°ì—…ëª…', 'N/A')}, ë‚ ì§œ={m.get('ë‚ ì§œ', 'N/A')}, "
            f"ì¹´í…Œê³ ë¦¬={m.get('ë¬¸ì„œ_ì¹´í…Œê³ ë¦¬', 'N/A')}, ì´ë²¤íŠ¸={m.get('ì£¼ìš”_ì´ë²¤íŠ¸', 'N/A')})\n{d.page_content}\n"
        )
    lines.append("ë‹µë³€:")
    return "\n".join(lines)

# 8) ë‹µë³€ ìƒì„±
def generate_with_llm(llm: ChatOpenAI, prompt: str) -> str:
    resp = llm.invoke(prompt)
    return resp.content.strip()

# 9) PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
def extract_text_from_pdf(file) -> str:
    doc = pymupdf.open(stream=file.read(), filetype="pdf")
    text = ""
    for page in doc:
        text += page.get_text()
    return text

# 10) Streamlit UI
def main():
    st.set_page_config(page_title="ğŸ¤– íˆ¬ì ì–´ì‹œìŠ¤í„´íŠ¸ (RAG)", page_icon="ğŸ¤–", layout="centered")
    st.title("ğŸ¤– íˆ¬ì ì–´ì‹œìŠ¤í„´íŠ¸ (RAG)")

    # OpenAI ë¡œë”©
    if "llm" not in st.session_state:
        st.session_state.llm = load_openai_llm("gpt-4o-mini", temperature=0.0)

    # DB ê¸°ë°˜ ë¬¸ì„œ ë¡œë“œ
    use_db = True
    try:
        db_texts, db_metadatas = load_documents_from_sqlite(DB_PATH)
    except FileNotFoundError as e:
        st.warning("DBë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        db_texts, db_metadatas = [], []

    # PDF ì—…ë¡œë“œ
    uploaded_pdf = st.file_uploader("ğŸ“„ ë¶„ì„í•  PDF ì—…ë¡œë“œ", type=["pdf"])
    if uploaded_pdf:
        pdf_text = extract_text_from_pdf(uploaded_pdf)
        if pdf_text.strip() == "":
            st.warning("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
            chunks = splitter.split_text(pdf_text)
            metadatas = [{"source": f"user_pdf_{i}"} for i in range(len(chunks))]
            st.session_state.retriever = build_ensemble_retriever(chunks, metadatas)
            st.info("ğŸ§  PDF ê¸°ë°˜ ë¬¸ì„œë¡œ ê²€ìƒ‰ë©ë‹ˆë‹¤.")
            use_db = False

    if use_db and "retriever" not in st.session_state:
        st.session_state.retriever = build_ensemble_retriever(db_texts, db_metadatas)

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    user_input = st.chat_input("ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”.")
    if user_input:
        st.chat_message("user").markdown(user_input)
        st.session_state.messages.append({"role": "user", "content": user_input})

        docs = search(user_input.strip(), st.session_state.retriever)
        if not docs:
            answer = "ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        else:
            prompt = build_prompt(user_input.strip(), docs)
            answer = generate_with_llm(st.session_state.llm, prompt)

        st.chat_message("assistant").markdown(answer)
        st.session_state.messages.append({"role": "assistant", "content": answer})

        with st.expander("ğŸ” ì‚¬ìš©í•œ ìë£Œ(ê²€ìƒ‰ ê²°ê³¼) ë³´ê¸°", expanded=False):
            if not docs:
                st.markdown("_ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ_")
            else:
                for i, d in enumerate(docs, 1):
                    m = d.metadata
                    st.markdown(
                        f"**[ë¬¸ì„œ{i}]** (source={m.get('source')}, ê¸°ì—…ëª…={m.get('ê¸°ì—…ëª…', '-')}, ë‚ ì§œ={m.get('ë‚ ì§œ', '-')}, "
                        f"ì¹´í…Œê³ ë¦¬={m.get('ë¬¸ì„œ_ì¹´í…Œê³ ë¦¬', '-')}, ì´ë²¤íŠ¸={m.get('ì£¼ìš”_ì´ë²¤íŠ¸', '-')})\n\n"
                        f"{d.page_content}"
                    )

# 11) ì•± ì‹¤í–‰
if __name__ == "__main__":
    main()
